{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5a4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python: c:\\Users\\61459\\anaconda3\\python.exe\n",
      "Torch: 2.8.0+cpu | CUDA available: False\n",
      "Binary classes: ['waste', 'recycling']\n",
      "Train label counts (0=waste, 1=recycling): {1: 3824, 0: 219}\n",
      "CrossEntropy weights: [9.23059368133545, 0.5286349654197693]  | BCE pos_weight (pos=1): 0.0572698749601841\n",
      "Batch shapes: torch.Size([64, 3, 224, 224]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# preprocessing Gary Thung trashnet.py dataset \n",
    "# --- Binary TrashNet dataloaders with minority-only extra aug & class imbalance handling ---\n",
    "# --- BASIC SETUP: make sure Python path and required packages are ready ---\n",
    "import sys, subprocess, pkgutil\n",
    "print(\"Using Python:\", sys.executable)\n",
    "\n",
    "# Install/upgrade pip and make sure torch/torchvision/kagglehub are available in THIS Python\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"kagglehub\"])\n",
    "\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Install light deps if missing (safe in notebooks)\n",
    "import sys, subprocess, importlib\n",
    "def _ensure(pkg, pip_name=None):\n",
    "    pip_name = pip_name or pkg\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "\n",
    "_ensure(\"datasets\")\n",
    "_ensure(\"PIL\", \"pillow\")\n",
    "\n",
    "import os, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Features\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    image_size: int = 224\n",
    "    val_pct: float = 0.10\n",
    "    test_pct: float = 0.10\n",
    "    seed: int = 42\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 0\n",
    "    pin_memory: bool = torch.cuda.is_available()\n",
    "    persistent_workers: bool = False\n",
    "    augment: bool = True\n",
    "    minority_extra_aug: bool = True  # extra aug only for the minority class\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# ---------------- Utils ----------------\n",
    "def _set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _pick_base_split(ds: DatasetDict) -> Dataset:\n",
    "    return ds[\"train\"] if \"train\" in ds else ds[next(iter(ds.keys()))]\n",
    "\n",
    "def _stratified_splits(base: Dataset, val_pct: float, test_pct: float, seed: int) -> DatasetDict:\n",
    "    assert 0 < val_pct < 1 and 0 < test_pct < 1 and val_pct + test_pct < 1\n",
    "    holdout_pct = val_pct + test_pct\n",
    "    tmp = base.train_test_split(test_size=holdout_pct, stratify_by_column=\"label\", seed=seed)\n",
    "    train, holdout = tmp[\"train\"], tmp[\"test\"]\n",
    "    test_frac_of_holdout = test_pct / (val_pct + test_pct)\n",
    "    hold = holdout.train_test_split(test_size=test_frac_of_holdout, stratify_by_column=\"label\", seed=seed)\n",
    "    return DatasetDict(train=train, val=hold[\"train\"], test=hold[\"test\"])\n",
    "\n",
    "def _class_names_from_features(feats: Features) -> List[str]:\n",
    "    if \"label\" in feats and hasattr(feats[\"label\"], \"names\") and feats[\"label\"].names:\n",
    "        return list(feats[\"label\"].names)\n",
    "    return []\n",
    "\n",
    "# ------------- 6 → 2 mapping -------------\n",
    "RECYCLE_NAMES = {\"glass\", \"paper\", \"cardboard\", \"plastic\", \"metal\"}\n",
    "WASTE_NAMES   = {\"trash\"}  # maps to class 0\n",
    "\n",
    "def _to_binary(ds: DatasetDict, feats: Features) -> DatasetDict:\n",
    "    orig = _class_names_from_features(feats)\n",
    "    recycle_ids = {i for i, n in enumerate(orig) if n in RECYCLE_NAMES}\n",
    "    waste_ids   = {i for i, n in enumerate(orig) if n in WASTE_NAMES}\n",
    "    if not recycle_ids or not waste_ids:\n",
    "        raise RuntimeError(f\"Could not build mapping from names {orig}\")\n",
    "\n",
    "    def map_fn(ex):\n",
    "        lid = int(ex[\"label\"])\n",
    "        ex[\"label\"] = 1 if lid in recycle_ids else 0  # 1=recycling, 0=waste\n",
    "        ex[\"orig_label\"] = lid\n",
    "        return ex\n",
    "\n",
    "    out = DatasetDict()\n",
    "    for k in ds.keys():\n",
    "        out[k] = ds[k].map(map_fn, desc=f\"Map 6->2 for {k}\")\n",
    "    return out\n",
    "\n",
    "# ------------- Transforms (fixed order) -------------\n",
    "def _build_transform_parts(image_size: int, augment: bool):\n",
    "    # PIL → PIL\n",
    "    pre = T.Compose([\n",
    "        T.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "        T.Resize((image_size, image_size), interpolation=InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "    # works on PIL or tensor, we keep it while still PIL\n",
    "    aug_common_pil = T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomApply([T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)], p=0.5),\n",
    "        T.RandomAffine(degrees=12, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    ]) if augment else T.Compose([])\n",
    "\n",
    "    # EXTRA aug for the minority class while still PIL\n",
    "    extra_minority_pil = T.Compose([\n",
    "        T.RandomApply([T.RandomPerspective(distortion_scale=0.3)], p=0.35),\n",
    "        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.35),\n",
    "    ]) if augment else T.Compose([])\n",
    "\n",
    "    # PIL → Tensor\n",
    "    to_tensor = T.ToTensor()\n",
    "    normalize = T.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "\n",
    "    # Tensor-only extra aug (RandomErasing MUST be after ToTensor)\n",
    "    extra_minority_tensor = T.RandomErasing(p=0.25, scale=(0.02, 0.08), ratio=(0.3, 3.3), value=\"random\")\n",
    "\n",
    "    return pre, aug_common_pil, extra_minority_pil, to_tensor, normalize, extra_minority_tensor\n",
    "\n",
    "def _set_transform_train(ds: Dataset, pre, aug_common_pil, extra_minority_pil,\n",
    "                         to_tensor, normalize, extra_minority_tensor,\n",
    "                         minority_label: Optional[int]):\n",
    "    # HuggingFace set_transform: batch dicts with list values\n",
    "    def _apply(batch):\n",
    "        imgs = batch[\"image\"]; labels = batch[\"label\"]\n",
    "        if not isinstance(imgs, list): imgs = [imgs]\n",
    "        if not isinstance(labels, list): labels = [labels]\n",
    "        out_imgs = []\n",
    "        for im, lbl in zip(imgs, labels):\n",
    "            x = pre(im)                  # PIL\n",
    "            x = aug_common_pil(x)        # PIL\n",
    "            if minority_label is not None and int(lbl) == int(minority_label):\n",
    "                x = extra_minority_pil(x)  # PIL (minority only)\n",
    "            x = to_tensor(x)             # Tensor\n",
    "            x = normalize(x)             # Tensor\n",
    "            if minority_label is not None and int(lbl) == int(minority_label):\n",
    "                x = extra_minority_tensor(x)  # Tensor-only (minority only)\n",
    "            out_imgs.append(x)\n",
    "        batch[\"image\"] = out_imgs\n",
    "        return batch\n",
    "    ds.set_transform(_apply)\n",
    "    return ds\n",
    "\n",
    "def _set_transform_eval(ds: Dataset, pre, to_tensor, normalize):\n",
    "    def _apply(batch):\n",
    "        imgs = batch[\"image\"]\n",
    "        if not isinstance(imgs, list): imgs = [imgs]\n",
    "        processed = []\n",
    "        for im in imgs:\n",
    "            x = pre(im)\n",
    "            x = to_tensor(x)\n",
    "            x = normalize(x)\n",
    "            processed.append(x)\n",
    "        batch[\"image\"] = processed\n",
    "        return batch\n",
    "    ds.set_transform(_apply)\n",
    "    return ds\n",
    "\n",
    "# ------------- Collate -------------\n",
    "def _collate_batch(batch):\n",
    "    imgs, labels = [], []\n",
    "    for b in batch:\n",
    "        img = b[\"image\"]; lbl = b[\"label\"]\n",
    "        if isinstance(img, list): img = img[0]\n",
    "        if isinstance(lbl, list): lbl = lbl[0]\n",
    "        imgs.append(img)\n",
    "        labels.append(int(lbl))\n",
    "    return torch.stack(imgs, dim=0), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# ------------- Balancing helpers -------------\n",
    "def _minority_label_from_counts(counts: Dict[int,int]) -> int:\n",
    "    return min(counts, key=counts.get)\n",
    "\n",
    "def _make_weighted_sampler(train_split: Dataset) -> WeightedRandomSampler:\n",
    "    labels = list(train_split[\"label\"])\n",
    "    c = Counter(labels)\n",
    "    w_per_class = {cls: 1.0 / cnt for cls, cnt in c.items()}\n",
    "    weights = torch.as_tensor([w_per_class[int(lbl)] for lbl in labels], dtype=torch.double)\n",
    "    return WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "def _loss_weights_from_counts(counts: Dict[int,int]) -> Dict[str, torch.Tensor]:\n",
    "    n0, n1 = counts.get(0, 0), counts.get(1, 0)  # 0=waste, 1=recycling\n",
    "    N = n0 + n1\n",
    "    ce_w = torch.tensor([N/(2*max(n0,1)), N/(2*max(n1,1))], dtype=torch.float)\n",
    "    # NOTE: For BCEWithLogitsLoss, \"positive\" is class 1 (recycling) here.\n",
    "    pos_weight = torch.tensor([max(n0,1)/max(n1,1)], dtype=torch.float)\n",
    "    return {\"ce\": ce_w, \"bce_pos\": pos_weight}\n",
    "\n",
    "# ------------- Public API -------------\n",
    "def build_dataloaders(\n",
    "    batch_size: int = 32,\n",
    "    image_size: int = 224,\n",
    "    val_pct: float = 0.10,\n",
    "    test_pct: float = 0.10,\n",
    "    seed: int = 42,\n",
    "    num_workers: Optional[int] = None,\n",
    "    augment: bool = True,\n",
    "    minority_extra_aug: bool = True,\n",
    "):\n",
    "    cfg = Config(\n",
    "        image_size=image_size, val_pct=val_pct, test_pct=test_pct, seed=seed,\n",
    "        batch_size=batch_size, num_workers=(num_workers if num_workers is not None else 0),\n",
    "        augment=augment, minority_extra_aug=minority_extra_aug,\n",
    "    )\n",
    "    cfg.persistent_workers = bool(cfg.num_workers and cfg.num_workers > 0)\n",
    "    _set_seed(cfg.seed)\n",
    "\n",
    "    # 1) Load & map to binary\n",
    "    ds_full = load_dataset(\"garythung/trashnet\")\n",
    "    feats = _pick_base_split(ds_full).features\n",
    "    ds_bin = _to_binary(ds_full, feats)\n",
    "\n",
    "    # 2) Stratified splits on binary label\n",
    "    splits = _stratified_splits(_pick_base_split(ds_bin), cfg.val_pct, cfg.test_pct, cfg.seed)\n",
    "\n",
    "    # 3) Balancing tools from TRAIN counts\n",
    "    train_counts = Counter(splits[\"train\"][\"label\"])\n",
    "    sampler = _make_weighted_sampler(splits[\"train\"])\n",
    "    balance = _loss_weights_from_counts(train_counts)\n",
    "    minority_lbl = _minority_label_from_counts(train_counts) if cfg.minority_extra_aug else None\n",
    "\n",
    "    # 4) Transforms (with fixed PIL→Tensor order and minority-only extras)\n",
    "    pre, aug_common_pil, extra_minority_pil, to_tensor, normalize, extra_minority_tensor = \\\n",
    "        _build_transform_parts(cfg.image_size, cfg.augment)\n",
    "\n",
    "    splits[\"train\"] = _set_transform_train(\n",
    "        splits[\"train\"], pre, aug_common_pil, extra_minority_pil, to_tensor, normalize,\n",
    "        extra_minority_tensor, minority_lbl\n",
    "    )\n",
    "    splits[\"val\"]   = _set_transform_eval(splits[\"val\"], pre, to_tensor, normalize)\n",
    "    splits[\"test\"]  = _set_transform_eval(splits[\"test\"], pre, to_tensor, normalize)\n",
    "\n",
    "    # 5) DataLoaders\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(\n",
    "            splits[\"train\"], batch_size=cfg.batch_size, sampler=sampler,\n",
    "            num_workers=cfg.num_workers, pin_memory=cfg.pin_memory,\n",
    "            persistent_workers=cfg.persistent_workers, collate_fn=_collate_batch\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            splits[\"val\"], batch_size=cfg.batch_size, shuffle=False,\n",
    "            num_workers=cfg.num_workers, pin_memory=cfg.pin_memory,\n",
    "            persistent_workers=cfg.persistent_workers, collate_fn=_collate_batch\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            splits[\"test\"], batch_size=cfg.batch_size, shuffle=False,\n",
    "            num_workers=cfg.num_workers, pin_memory=cfg.pin_memory,\n",
    "            persistent_workers=cfg.persistent_workers, collate_fn=_collate_batch\n",
    "        ),\n",
    "    }\n",
    "    class_names = [\"waste\", \"recycling\"]  # 0, 1\n",
    "    return loaders, class_names, balance, train_counts\n",
    "\n",
    "# ---------------- Smoke test (run this cell) ----------------\n",
    "loaders, classes, balance, counts = build_dataloaders(batch_size=64, image_size=224, num_workers=0)\n",
    "print(\"Binary classes:\", classes)\n",
    "print(\"Train label counts (0=waste, 1=recycling):\", dict(counts))\n",
    "print(\"CrossEntropy weights:\", balance[\"ce\"].tolist(), \" | BCE pos_weight (pos=1):\", float(balance[\"bce_pos\"]))\n",
    "xb, yb = next(iter(loaders[\"train\"]))\n",
    "print(\"Batch shapes:\", xb.shape, yb.shape)\n",
    "\n",
    "\n",
    "    #This will \n",
    "    # (a) serve roughly 50/50 batches via the sampler\n",
    "    # (b) weight the loss toward the rarer waste class\n",
    "    # (c) add extra aug only to the minority during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b10a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
