{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1e44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset root: C:\\Users\\22913\\.cache\\kagglehub\\datasets\\alistairking\\recyclable-and-household-waste-classification\\versions\\1\\images\\images\n",
      "Found 30 classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging']...\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from pathlib import Path\n",
    "\n",
    "# allow jpg + png\n",
    "def is_img(p): \n",
    "    p=str(p).lower()\n",
    "    return p.endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\"))\n",
    "\n",
    "# FIX: Point to the correct directory with /images/images\n",
    "base_path = Path(\"C:/Users/22913/.cache/kagglehub/datasets/alistairking/recyclable-and-household-waste-classification/versions/1\")\n",
    "candidates = [\n",
    "    base_path / \"images\" / \"images\",\n",
    "    base_path / \"images\",\n",
    "    base_path,\n",
    "]\n",
    "\n",
    "root = None\n",
    "for c in candidates:\n",
    "    if c.exists() and any(d.is_dir() for d in c.iterdir()):\n",
    "        root = c\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise FileNotFoundError(f\"Could not find image folder in: {base_path}\")\n",
    "\n",
    "print(f\"Using dataset root: {root}\")\n",
    "full = datasets.ImageFolder(root=str(root), transform=None, is_valid_file=is_img)\n",
    "print(f\"Found {len(full.classes)} classes: {full.classes[:5]}...\")\n",
    "targets = [y for _, y in full.samples]\n",
    "\n",
    "# stratified split\n",
    "tr_idx, va_idx = train_test_split(\n",
    "    np.arange(len(targets)), test_size=0.2, random_state=56, stratify=targets\n",
    ")\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "train_ds = Subset(datasets.ImageFolder(str(root), transform=train_tfms, is_valid_file=is_img), tr_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(str(root), transform=val_tfms,   is_valid_file=is_img), va_idx)\n",
    "\n",
    "# optional balancing\n",
    "counts = np.bincount(np.array(targets)[tr_idx])\n",
    "class_w = 1.0 / np.clip(counts, 1, None)\n",
    "weights = class_w[np.array(targets)[tr_idx]]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(tr_idx), replacement=True)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "num_classes = len(full.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d70239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Number of classes: 30\n",
      "✓ Total samples: 15000\n",
      "✓ Train samples: 12000\n",
      "✓ Val samples: 3000\n",
      "✓ No overlap: True\n",
      "✓ Duplicate images in sample (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset is loaded correctly\n",
    "print(f\"✓ Number of classes: {num_classes}\")\n",
    "print(f\"✓ Total samples: {len(full.samples)}\")\n",
    "print(f\"✓ Train samples: {len(tr_idx)}\")\n",
    "print(f\"✓ Val samples: {len(va_idx)}\")\n",
    "print(f\"✓ No overlap: {set(tr_idx).isdisjoint(set(va_idx))}\")\n",
    "\n",
    "# Quick check for duplicates (sample first 100 files)\n",
    "from hashlib import md5\n",
    "def quick_hash_check(indices, n=100):\n",
    "    hashes = set()\n",
    "    for i in indices[:n]:\n",
    "        path = full.samples[i][0]\n",
    "        with open(path, 'rb') as f:\n",
    "            hashes.add(md5(f.read()).hexdigest())\n",
    "    return hashes\n",
    "\n",
    "train_hashes = quick_hash_check(tr_idx.tolist())\n",
    "val_hashes = quick_hash_check(va_idx.tolist())\n",
    "duplicates = train_hashes & val_hashes\n",
    "print(f\"✓ Duplicate images in sample (should be 0): {len(duplicates)}\")\n",
    "\n",
    "if num_classes == 1:\n",
    "    print(\"⚠️  WARNING: Only 1 class detected - check dataset path!\")\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"⚠️  WARNING: Found {len(duplicates)} duplicate images between train/val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363e75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm, torch\n",
    "from torch import nn\n",
    "\n",
    "def make_model(name, num_classes):\n",
    "    m = timm.create_model(name, pretrained=True, drop_rate=0.2, drop_path_rate=0.1, num_classes=num_classes)\n",
    "    return m\n",
    "\n",
    "m_small = make_model(\"mobilenetv3_small_100\", num_classes)\n",
    "m_large = make_model(\"mobilenetv3_large_100\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20805df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_dl, val_dl, epochs=15, lr=5e-4, wd=0.05, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=3)\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs-3)\n",
    "    sched = torch.optim.lr_scheduler.SequentialLR(opt, [warmup, cosine], milestones=[3])\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scaler = GradScaler('cuda', enabled=(device.startswith(\"cuda\")))\n",
    "    best = {\"f1\": -1, \"state\": None}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for x,y in train_dl:\n",
    "            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast('cuda', enabled=(device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                loss = crit(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        sched.step()\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_dl:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                logits = model(x)\n",
    "                preds.append(logits.argmax(1).cpu())\n",
    "                gts.append(y)\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import f1_score, accuracy_score\n",
    "        p = torch.cat(preds).numpy(); g = torch.cat(gts).numpy()\n",
    "        f1 = f1_score(g, p, average=\"macro\"); acc = accuracy_score(g, p)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"f1\": f1, \"state\": model.state_dict()}\n",
    "        print(f\"ep {ep+1}: acc {acc:.4f}  macroF1 {f1:.4f}\")\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b5a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Python\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\22913\\.cache\\huggingface\\hub\\models--timm--vit_tiny_patch16_224.augreg_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MobileNetV3-Small params: 1.55M\n",
      "✓ MobileNetV3-Large params: 4.24M\n",
      "✓ ViT-Tiny params: 5.53M\n",
      "✓ ViT-Small params: 21.68M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\22913\\.cache\\huggingface\\hub\\models--timm--vit_small_patch16_224.augreg_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Add Vision Transformer models\n",
    "vit_tiny = make_model(\"vit_tiny_patch16_224\", num_classes)\n",
    "vit_small = make_model(\"vit_small_patch16_224\", num_classes)\n",
    "\n",
    "print(f\"✓ MobileNetV3-Small params: {sum(p.numel() for p in m_small.parameters())/1e6:.2f}M\")\n",
    "print(f\"✓ MobileNetV3-Large params: {sum(p.numel() for p in m_large.parameters())/1e6:.2f}M\")\n",
    "print(f\"✓ ViT-Tiny params: {sum(p.numel() for p in vit_tiny.parameters())/1e6:.2f}M\")\n",
    "print(f\"✓ ViT-Small params: {sum(p.numel() for p in vit_small.parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d961af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1: acc 0.5863  macroF1 0.5823\n",
      "ep 2: acc 0.7217  macroF1 0.7234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.7083  macroF1 0.7077\n",
      "ep 4: acc 0.7120  macroF1 0.7156\n",
      "ep 5: acc 0.7173  macroF1 0.7183\n",
      "ep 6: acc 0.7157  macroF1 0.7283\n",
      "ep 7: acc 0.7737  macroF1 0.7765\n",
      "ep 8: acc 0.8003  macroF1 0.8009\n",
      "ep 9: acc 0.8093  macroF1 0.8089\n",
      "ep 10: acc 0.8117  macroF1 0.8107\n",
      "ep 11: acc 0.8130  macroF1 0.8145\n",
      "ep 12: acc 0.8453  macroF1 0.8454\n",
      "ep 13: acc 0.8427  macroF1 0.8430\n",
      "ep 14: acc 0.8620  macroF1 0.8619\n",
      "ep 15: acc 0.8507  macroF1 0.8507\n",
      "ep 16: acc 0.8577  macroF1 0.8576\n",
      "ep 17: acc 0.8673  macroF1 0.8670\n",
      "ep 18: acc 0.8647  macroF1 0.8642\n",
      "ep 19: acc 0.8660  macroF1 0.8662\n",
      "ep 20: acc 0.8640  macroF1 0.8639\n",
      "ep 1: acc 0.0333  macroF1 0.0022\n",
      "ep 2: acc 0.0333  macroF1 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.0333  macroF1 0.0022\n",
      "ep 4: acc 0.0333  macroF1 0.0022\n",
      "ep 5: acc 0.0333  macroF1 0.0022\n",
      "ep 6: acc 0.0333  macroF1 0.0022\n",
      "ep 7: acc 0.0333  macroF1 0.0022\n",
      "ep 8: acc 0.0333  macroF1 0.0022\n",
      "ep 9: acc 0.0333  macroF1 0.0022\n",
      "ep 10: acc 0.0333  macroF1 0.0022\n",
      "ep 11: acc 0.0333  macroF1 0.0022\n",
      "ep 12: acc 0.0333  macroF1 0.0022\n",
      "ep 13: acc 0.0333  macroF1 0.0022\n",
      "ep 14: acc 0.0333  macroF1 0.0022\n",
      "ep 15: acc 0.0333  macroF1 0.0022\n",
      "ep 16: acc 0.0333  macroF1 0.0022\n",
      "ep 17: acc 0.0333  macroF1 0.0022\n",
      "ep 18: acc 0.0333  macroF1 0.0022\n",
      "ep 19: acc 0.0333  macroF1 0.0022\n",
      "ep 20: acc 0.0333  macroF1 0.0022\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "small = train_model(m_small, train_dl, val_dl, epochs=20, device=device)\n",
    "large = train_model(m_large, train_dl, val_dl, epochs=20, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ViT-Tiny...\n",
      "ep 1: acc 0.5583  macroF1 0.5322\n",
      "ep 2: acc 0.7740  macroF1 0.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.7943  macroF1 0.7901\n",
      "ep 4: acc 0.8073  macroF1 0.8065\n",
      "ep 5: acc 0.8263  macroF1 0.8266\n",
      "ep 6: acc 0.8310  macroF1 0.8291\n",
      "ep 7: acc 0.8460  macroF1 0.8466\n",
      "ep 8: acc 0.8427  macroF1 0.8413\n",
      "ep 9: acc 0.8560  macroF1 0.8548\n",
      "ep 10: acc 0.8583  macroF1 0.8567\n",
      "ep 11: acc 0.8633  macroF1 0.8624\n",
      "ep 12: acc 0.8647  macroF1 0.8637\n",
      "ep 13: acc 0.8713  macroF1 0.8711\n",
      "ep 14: acc 0.8793  macroF1 0.8776\n",
      "ep 15: acc 0.8740  macroF1 0.8734\n",
      "ep 16: acc 0.8830  macroF1 0.8813\n",
      "ep 17: acc 0.8820  macroF1 0.8812\n",
      "ep 18: acc 0.8900  macroF1 0.8896\n",
      "ep 19: acc 0.8867  macroF1 0.8858\n",
      "ep 20: acc 0.8877  macroF1 0.8870\n",
      "\n",
      "Training ViT-Small...\n",
      "ep 1: acc 0.7557  macroF1 0.7481\n",
      "ep 2: acc 0.8303  macroF1 0.8242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.8410  macroF1 0.8363\n",
      "ep 4: acc 0.8230  macroF1 0.8236\n",
      "ep 5: acc 0.8440  macroF1 0.8402\n",
      "ep 6: acc 0.8530  macroF1 0.8515\n"
     ]
    }
   ],
   "source": [
    "# Train Vision Transformer models\n",
    "# Note: ViT may need slightly lower learning rate and longer warmup\n",
    "print(\"Training ViT-Tiny...\")\n",
    "vit_tiny_trained = train_model(vit_tiny, train_dl, val_dl, epochs=20, lr=3e-4, wd=0.05, device=device)\n",
    "\n",
    "print(\"\\nTraining ViT-Small...\")\n",
    "vit_small_trained = train_model(vit_small, train_dl, val_dl, epochs=20, lr=3e-4, wd=0.05, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ce81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference speed across all models\n",
    "import time, torch\n",
    "def ms_per_image(model, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    x = torch.randn(1,3,224,224, device=device)\n",
    "    # warmup\n",
    "    for _ in range(5): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(50): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    return 1000*(time.time()-t0)/50\n",
    "\n",
    "print(\"=== Inference Speed Comparison ===\")\n",
    "print(f\"MobileNetV3-Small: {ms_per_image(small, device):.2f} ms/img\")\n",
    "print(f\"MobileNetV3-Large: {ms_per_image(large, device):.2f} ms/img\")\n",
    "print(f\"ViT-Tiny:          {ms_per_image(vit_tiny_trained, device):.2f} ms/img\")\n",
    "print(f\"ViT-Small:         {ms_per_image(vit_small_trained, device):.2f} ms/img\")\n",
    "\n",
    "# Get final validation metrics for comparison\n",
    "def get_final_metrics(model, val_dl, device):\n",
    "    model.eval().to(device)\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_dl:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            preds.append(logits.argmax(1).cpu())\n",
    "            gts.append(y)\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    p = torch.cat(preds).numpy(); g = torch.cat(gts).numpy()\n",
    "    return accuracy_score(g, p), f1_score(g, p, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== Final Validation Performance ===\")\n",
    "for name, model in [(\"MobileNetV3-Small\", small), (\"MobileNetV3-Large\", large), \n",
    "                     (\"ViT-Tiny\", vit_tiny_trained), (\"ViT-Small\", vit_small_trained)]:\n",
    "    acc, f1 = get_final_metrics(model, val_dl, device)\n",
    "    params = sum(p.numel() for p in model.parameters())/1e6\n",
    "    print(f\"{name:20s} | Acc: {acc:.4f} | F1: {f1:.4f} | Params: {params:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Vision Transformer models (if they perform well)\n",
    "# Save ViT-Tiny\n",
    "torch.save({\n",
    "    \"model\": \"vit_tiny_patch16_224\",\n",
    "    \"classes\": full.classes,\n",
    "    \"state_dict\": vit_tiny_trained.state_dict()\n",
    "}, \"vit_tiny.pt\")\n",
    "\n",
    "# Save ViT-Small\n",
    "torch.save({\n",
    "    \"model\": \"vit_small_patch16_224\",\n",
    "    \"classes\": full.classes,\n",
    "    \"state_dict\": vit_small_trained.state_dict()\n",
    "}, \"vit_small.pt\")\n",
    "\n",
    "# Optional: Export best performing ViT to ONNX\n",
    "# Uncomment to export ViT-Tiny to ONNX\n",
    "# dummy = torch.randn(1,3,224,224)\n",
    "# vit_tiny_trained.eval().cpu()\n",
    "# torch.onnx.export(vit_tiny_trained, dummy, \"vit_tiny.onnx\", \n",
    "#                   input_names=[\"input\"], output_names=[\"logits\"], opset_version=17)\n",
    "\n",
    "print(\"✓ Saved ViT models as vit_tiny.pt and vit_small.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small ms/img: 11.293988227844238\n",
      "Large ms/img: 12.904634475708008\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "def ms_per_image(model, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    x = torch.randn(1,3,224,224, device=device)\n",
    "    # warmup\n",
    "    for _ in range(5): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(50): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    return 1000*(time.time()-t0)/50\n",
    "\n",
    "print(\"Small ms/img:\", ms_per_image(small, device))\n",
    "print(\"Large ms/img:\", ms_per_image(large, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\":\"mobilenetv3_small_100\",\"classes\":full.classes,\"state_dict\":small.state_dict()}, \"mobilenetv3_small.pt\")\n",
    "\n",
    "# ONNX for CPU apps\n",
    "dummy = torch.randn(1,3,224,224)\n",
    "small.eval().cpu()\n",
    "torch.onnx.export(small, dummy, \"mobilenetv3_small.onnx\", input_names=[\"input\"], output_names=[\"logits\"], opset_version=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ddbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
