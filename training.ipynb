{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1e44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg123\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset root: C:\\Users\\hoang\\.cache\\kagglehub\\datasets\\alistairking\\recyclable-and-household-waste-classification\\versions\\1\\images\\images\n",
      "Found 30 classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging']...\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from pathlib import Path\n",
    "\n",
    "# allow jpg + png\n",
    "def is_img(p): \n",
    "    p=str(p).lower()\n",
    "    return p.endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\"))\n",
    "\n",
    "# FIX: Point to the correct directory with /images/images\n",
    "base_path = Path(\"C:/Users/hoang/.cache/kagglehub/datasets/alistairking/recyclable-and-household-waste-classification/versions/1\")\n",
    "candidates = [\n",
    "    base_path / \"images\" / \"images\",\n",
    "    base_path / \"images\",\n",
    "    base_path,\n",
    "]\n",
    "\n",
    "root = None\n",
    "for c in candidates:\n",
    "    if c.exists() and any(d.is_dir() for d in c.iterdir()):\n",
    "        root = c\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise FileNotFoundError(f\"Could not find image folder in: {base_path}\")\n",
    "\n",
    "print(f\"Using dataset root: {root}\")\n",
    "full = datasets.ImageFolder(root=str(root), transform=None, is_valid_file=is_img)\n",
    "print(f\"Found {len(full.classes)} classes: {full.classes[:5]}...\")\n",
    "targets = [y for _, y in full.samples]\n",
    "\n",
    "# stratified split\n",
    "tr_idx, va_idx = train_test_split(\n",
    "    np.arange(len(targets)), test_size=0.2, random_state=56, stratify=targets\n",
    ")\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "train_ds = Subset(datasets.ImageFolder(str(root), transform=train_tfms, is_valid_file=is_img), tr_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(str(root), transform=val_tfms,   is_valid_file=is_img), va_idx)\n",
    "\n",
    "# optional balancing\n",
    "counts = np.bincount(np.array(targets)[tr_idx])\n",
    "class_w = 1.0 / np.clip(counts, 1, None)\n",
    "weights = class_w[np.array(targets)[tr_idx]]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(tr_idx), replacement=True)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "num_classes = len(full.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d70239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Number of classes: 30\n",
      "✓ Total samples: 15000\n",
      "✓ Train samples: 12000\n",
      "✓ Val samples: 3000\n",
      "✓ No overlap: True\n",
      "✓ Duplicate images in sample (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset is loaded correctly\n",
    "print(f\"✓ Number of classes: {num_classes}\")\n",
    "print(f\"✓ Total samples: {len(full.samples)}\")\n",
    "print(f\"✓ Train samples: {len(tr_idx)}\")\n",
    "print(f\"✓ Val samples: {len(va_idx)}\")\n",
    "print(f\"✓ No overlap: {set(tr_idx).isdisjoint(set(va_idx))}\")\n",
    "\n",
    "# Quick check for duplicates (sample first 100 files)\n",
    "from hashlib import md5\n",
    "def quick_hash_check(indices, n=100):\n",
    "    hashes = set()\n",
    "    for i in indices[:n]:\n",
    "        path = full.samples[i][0]\n",
    "        with open(path, 'rb') as f:\n",
    "            hashes.add(md5(f.read()).hexdigest())\n",
    "    return hashes\n",
    "\n",
    "train_hashes = quick_hash_check(tr_idx.tolist())\n",
    "val_hashes = quick_hash_check(va_idx.tolist())\n",
    "duplicates = train_hashes & val_hashes\n",
    "print(f\"✓ Duplicate images in sample (should be 0): {len(duplicates)}\")\n",
    "\n",
    "if num_classes == 1:\n",
    "    print(\"⚠️  WARNING: Only 1 class detected - check dataset path!\")\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"⚠️  WARNING: Found {len(duplicates)} duplicate images between train/val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363e75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm, torch\n",
    "from torch import nn\n",
    "\n",
    "def make_model(name, num_classes):\n",
    "    m = timm.create_model(name, pretrained=True, drop_rate=0.2, drop_path_rate=0.1, num_classes=num_classes)\n",
    "    return m\n",
    "\n",
    "m_small = make_model(\"mobilenetv3_small_100\", num_classes)\n",
    "m_large = make_model(\"mobilenetv3_large_100\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20805df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_dl, val_dl, epochs=15, lr=5e-4, wd=0.05, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=3)\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs-3)\n",
    "    sched = torch.optim.lr_scheduler.SequentialLR(opt, [warmup, cosine], milestones=[3])\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scaler = GradScaler('cuda', enabled=(device.startswith(\"cuda\")))\n",
    "    best = {\"f1\": -1, \"state\": None}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for x,y in train_dl:\n",
    "            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast('cuda', enabled=(device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                loss = crit(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        sched.step()\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_dl:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                logits = model(x)\n",
    "                preds.append(logits.argmax(1).cpu())\n",
    "                gts.append(y)\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import f1_score, accuracy_score\n",
    "        p = torch.cat(preds).numpy(); g = torch.cat(gts).numpy()\n",
    "        f1 = f1_score(g, p, average=\"macro\"); acc = accuracy_score(g, p)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"f1\": f1, \"state\": model.state_dict()}\n",
    "        print(f\"ep {ep+1}: acc {acc:.4f}  macroF1 {f1:.4f}\")\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b5a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MobileNetV3-Small params: 1.55M\n",
      "✓ MobileNetV3-Large params: 4.24M\n",
      "✓ ViT-Tiny params: 5.53M\n",
      "✓ ViT-Small params: 21.68M\n"
     ]
    }
   ],
   "source": [
    "# Add Vision Transformer models\n",
    "vit_tiny = make_model(\"vit_tiny_patch16_224\", num_classes)\n",
    "vit_small = make_model(\"vit_small_patch16_224\", num_classes)\n",
    "\n",
    "print(f\"✓ MobileNetV3-Small params: {sum(p.numel() for p in m_small.parameters())/1e6:.2f}M\")\n",
    "print(f\"✓ MobileNetV3-Large params: {sum(p.numel() for p in m_large.parameters())/1e6:.2f}M\")\n",
    "print(f\"✓ ViT-Tiny params: {sum(p.numel() for p in vit_tiny.parameters())/1e6:.2f}M\")\n",
    "print(f\"✓ ViT-Small params: {sum(p.numel() for p in vit_small.parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d961af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1: acc 0.5367  macroF1 0.5302\n",
      "ep 2: acc 0.6903  macroF1 0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg123\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.7007  macroF1 0.6969\n",
      "ep 4: acc 0.6977  macroF1 0.6912\n",
      "ep 5: acc 0.7457  macroF1 0.7425\n",
      "ep 6: acc 0.7717  macroF1 0.7709\n",
      "ep 7: acc 0.7870  macroF1 0.7881\n",
      "ep 8: acc 0.7850  macroF1 0.7842\n",
      "ep 9: acc 0.7993  macroF1 0.7992\n",
      "ep 10: acc 0.8243  macroF1 0.8213\n",
      "ep 11: acc 0.8290  macroF1 0.8294\n",
      "ep 12: acc 0.8347  macroF1 0.8351\n",
      "ep 13: acc 0.8387  macroF1 0.8391\n",
      "ep 14: acc 0.8457  macroF1 0.8453\n",
      "ep 15: acc 0.8597  macroF1 0.8587\n",
      "ep 16: acc 0.8590  macroF1 0.8588\n",
      "ep 17: acc 0.8600  macroF1 0.8597\n",
      "ep 18: acc 0.8627  macroF1 0.8623\n",
      "ep 19: acc 0.8673  macroF1 0.8670\n",
      "ep 20: acc 0.8657  macroF1 0.8655\n",
      "ep 1: acc 0.5953  macroF1 0.5928\n",
      "ep 2: acc 0.7707  macroF1 0.7695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg123\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.8290  macroF1 0.8282\n",
      "ep 4: acc 0.8410  macroF1 0.8401\n",
      "ep 5: acc 0.8583  macroF1 0.8588\n",
      "ep 6: acc 0.8557  macroF1 0.8553\n",
      "ep 7: acc 0.8670  macroF1 0.8659\n",
      "ep 8: acc 0.8773  macroF1 0.8770\n",
      "ep 9: acc 0.8673  macroF1 0.8674\n",
      "ep 10: acc 0.8727  macroF1 0.8724\n",
      "ep 11: acc 0.8730  macroF1 0.8728\n",
      "ep 12: acc 0.8770  macroF1 0.8768\n",
      "ep 13: acc 0.8823  macroF1 0.8816\n",
      "ep 14: acc 0.8803  macroF1 0.8802\n",
      "ep 15: acc 0.8773  macroF1 0.8771\n",
      "ep 16: acc 0.8733  macroF1 0.8734\n",
      "ep 17: acc 0.8790  macroF1 0.8785\n",
      "ep 18: acc 0.8747  macroF1 0.8746\n",
      "ep 19: acc 0.8807  macroF1 0.8800\n",
      "ep 20: acc 0.8800  macroF1 0.8793\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "small = train_model(m_small, train_dl, val_dl, epochs=20, device=device)\n",
    "large = train_model(m_large, train_dl, val_dl, epochs=20, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f9d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ViT-Tiny...\n",
      "ep 1: acc 0.6180  macroF1 0.6017\n",
      "ep 2: acc 0.7900  macroF1 0.7871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg123\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.7993  macroF1 0.8004\n",
      "ep 4: acc 0.7923  macroF1 0.7814\n",
      "ep 5: acc 0.8083  macroF1 0.8054\n",
      "ep 6: acc 0.8290  macroF1 0.8288\n",
      "ep 7: acc 0.8433  macroF1 0.8427\n",
      "ep 8: acc 0.8423  macroF1 0.8398\n",
      "ep 9: acc 0.8423  macroF1 0.8428\n",
      "ep 10: acc 0.8677  macroF1 0.8680\n",
      "ep 11: acc 0.8530  macroF1 0.8513\n",
      "ep 12: acc 0.8623  macroF1 0.8614\n",
      "ep 13: acc 0.8753  macroF1 0.8744\n",
      "ep 14: acc 0.8727  macroF1 0.8719\n",
      "ep 15: acc 0.8843  macroF1 0.8838\n",
      "ep 16: acc 0.8830  macroF1 0.8820\n",
      "ep 17: acc 0.8823  macroF1 0.8816\n",
      "ep 18: acc 0.8840  macroF1 0.8832\n",
      "ep 19: acc 0.8857  macroF1 0.8853\n",
      "ep 20: acc 0.8867  macroF1 0.8863\n",
      "\n",
      "Training ViT-Small...\n",
      "ep 1: acc 0.7607  macroF1 0.7554\n",
      "ep 2: acc 0.8480  macroF1 0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg123\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.8463  macroF1 0.8445\n",
      "ep 4: acc 0.8247  macroF1 0.8155\n",
      "ep 5: acc 0.8470  macroF1 0.8414\n",
      "ep 6: acc 0.8397  macroF1 0.8381\n",
      "ep 7: acc 0.8583  macroF1 0.8582\n",
      "ep 8: acc 0.8563  macroF1 0.8553\n",
      "ep 9: acc 0.8697  macroF1 0.8692\n",
      "ep 10: acc 0.8817  macroF1 0.8817\n",
      "ep 11: acc 0.8710  macroF1 0.8716\n",
      "ep 12: acc 0.8743  macroF1 0.8743\n",
      "ep 13: acc 0.8880  macroF1 0.8877\n",
      "ep 14: acc 0.8900  macroF1 0.8893\n",
      "ep 15: acc 0.8943  macroF1 0.8934\n",
      "ep 16: acc 0.8893  macroF1 0.8882\n",
      "ep 17: acc 0.8930  macroF1 0.8929\n",
      "ep 18: acc 0.8910  macroF1 0.8903\n",
      "ep 19: acc 0.8927  macroF1 0.8922\n",
      "ep 20: acc 0.8937  macroF1 0.8932\n"
     ]
    }
   ],
   "source": [
    "# Train Vision Transformer models\n",
    "# Note: ViT may need slightly lower learning rate and longer warmup\n",
    "print(\"Training ViT-Tiny...\")\n",
    "vit_tiny_trained = train_model(vit_tiny, train_dl, val_dl, epochs=20, lr=3e-4, wd=0.05, device=device)\n",
    "\n",
    "print(\"\\nTraining ViT-Small...\")\n",
    "vit_small_trained = train_model(vit_small, train_dl, val_dl, epochs=20, lr=3e-4, wd=0.05, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "942ce81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Speed Comparison ===\n",
      "MobileNetV3-Small: 5.70 ms/img\n",
      "MobileNetV3-Large: 6.50 ms/img\n",
      "ViT-Tiny:          6.02 ms/img\n",
      "ViT-Small:         5.42 ms/img\n",
      "\n",
      "=== Final Validation Performance ===\n",
      "MobileNetV3-Small    | Acc: 0.8657 | F1: 0.8655 | Params: 1.55M\n",
      "MobileNetV3-Large    | Acc: 0.8800 | F1: 0.8793 | Params: 4.24M\n",
      "ViT-Tiny             | Acc: 0.8867 | F1: 0.8863 | Params: 5.53M\n",
      "ViT-Small            | Acc: 0.8937 | F1: 0.8932 | Params: 21.68M\n"
     ]
    }
   ],
   "source": [
    "# Compare inference speed across all models\n",
    "import time, torch\n",
    "def ms_per_image(model, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    x = torch.randn(1,3,224,224, device=device)\n",
    "    # warmup\n",
    "    for _ in range(5): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(50): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    return 1000*(time.time()-t0)/50\n",
    "\n",
    "print(\"=== Inference Speed Comparison ===\")\n",
    "print(f\"MobileNetV3-Small: {ms_per_image(small, device):.2f} ms/img\")\n",
    "print(f\"MobileNetV3-Large: {ms_per_image(large, device):.2f} ms/img\")\n",
    "print(f\"ViT-Tiny:          {ms_per_image(vit_tiny_trained, device):.2f} ms/img\")\n",
    "print(f\"ViT-Small:         {ms_per_image(vit_small_trained, device):.2f} ms/img\")\n",
    "\n",
    "# Get final validation metrics for comparison\n",
    "def get_final_metrics(model, val_dl, device):\n",
    "    model.eval().to(device)\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_dl:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            preds.append(logits.argmax(1).cpu())\n",
    "            gts.append(y)\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    p = torch.cat(preds).numpy(); g = torch.cat(gts).numpy()\n",
    "    return accuracy_score(g, p), f1_score(g, p, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== Final Validation Performance ===\")\n",
    "for name, model in [(\"MobileNetV3-Small\", small), (\"MobileNetV3-Large\", large), \n",
    "                     (\"ViT-Tiny\", vit_tiny_trained), (\"ViT-Small\", vit_small_trained)]:\n",
    "    acc, f1 = get_final_metrics(model, val_dl, device)\n",
    "    params = sum(p.numel() for p in model.parameters())/1e6\n",
    "    print(f\"{name:20s} | Acc: {acc:.4f} | F1: {f1:.4f} | Params: {params:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf1b64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved ViT models as vit_tiny.pt and vit_small.pt\n"
     ]
    }
   ],
   "source": [
    "# Save Vision Transformer models (if they perform well)\n",
    "# Save ViT-Tiny\n",
    "torch.save({\n",
    "    \"model\": \"vit_tiny_patch16_224\",\n",
    "    \"classes\": full.classes,\n",
    "    \"state_dict\": vit_tiny_trained.state_dict()\n",
    "}, \"vit_tiny.pt\")\n",
    "\n",
    "# Save ViT-Small\n",
    "torch.save({\n",
    "    \"model\": \"vit_small_patch16_224\",\n",
    "    \"classes\": full.classes,\n",
    "    \"state_dict\": vit_small_trained.state_dict()\n",
    "}, \"vit_small.pt\")\n",
    "\n",
    "# Optional: Export best performing ViT to ONNX\n",
    "# Uncomment to export ViT-Tiny to ONNX\n",
    "# dummy = torch.randn(1,3,224,224)\n",
    "# vit_tiny_trained.eval().cpu()\n",
    "# torch.onnx.export(vit_tiny_trained, dummy, \"vit_tiny.onnx\", \n",
    "#                   input_names=[\"input\"], output_names=[\"logits\"], opset_version=17)\n",
    "\n",
    "print(\"✓ Saved ViT models as vit_tiny.pt and vit_small.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caeb5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small ms/img: 6.42244815826416\n",
      "Large ms/img: 7.686314582824707\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "def ms_per_image(model, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    x = torch.randn(1,3,224,224, device=device)\n",
    "    # warmup\n",
    "    for _ in range(5): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(50): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    return 1000*(time.time()-t0)/50\n",
    "\n",
    "print(\"Small ms/img:\", ms_per_image(small, device))\n",
    "print(\"Large ms/img:\", ms_per_image(large, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c5a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\":\"mobilenetv3_small_100\",\"classes\":full.classes,\"state_dict\":small.state_dict()}, \"mobilenetv3_small.pt\")\n",
    "\n",
    "# ONNX for CPU apps\n",
    "dummy = torch.randn(1,3,224,224)\n",
    "small.eval().cpu()\n",
    "torch.onnx.export(small, dummy, \"mobilenetv3_small.onnx\", input_names=[\"input\"], output_names=[\"logits\"], opset_version=17)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
