{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1e44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset root: C:\\Users\\hoang\\.cache\\kagglehub\\datasets\\alistairking\\recyclable-and-household-waste-classification\\versions\\1\\images\\images\n",
      "Found 30 classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging']...\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from pathlib import Path\n",
    "\n",
    "# allow jpg + png\n",
    "def is_img(p): \n",
    "    p=str(p).lower()\n",
    "    return p.endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\"))\n",
    "\n",
    "# FIX: Point to the correct directory with /images/images\n",
    "base_path = Path(\"C:/Users/hoang/.cache/kagglehub/datasets/alistairking/recyclable-and-household-waste-classification/versions/1\")\n",
    "candidates = [\n",
    "    base_path / \"images\" / \"images\",\n",
    "    base_path / \"images\",\n",
    "    base_path,\n",
    "]\n",
    "\n",
    "root = None\n",
    "for c in candidates:\n",
    "    if c.exists() and any(d.is_dir() for d in c.iterdir()):\n",
    "        root = c\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise FileNotFoundError(f\"Could not find image folder in: {base_path}\")\n",
    "\n",
    "print(f\"Using dataset root: {root}\")\n",
    "full = datasets.ImageFolder(root=str(root), transform=None, is_valid_file=is_img)\n",
    "print(f\"Found {len(full.classes)} classes: {full.classes[:5]}...\")\n",
    "targets = [y for _, y in full.samples]\n",
    "\n",
    "# stratified split\n",
    "tr_idx, va_idx = train_test_split(\n",
    "    np.arange(len(targets)), test_size=0.2, random_state=56, stratify=targets\n",
    ")\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "train_ds = Subset(datasets.ImageFolder(str(root), transform=train_tfms, is_valid_file=is_img), tr_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(str(root), transform=val_tfms,   is_valid_file=is_img), va_idx)\n",
    "\n",
    "# optional balancing\n",
    "counts = np.bincount(np.array(targets)[tr_idx])\n",
    "class_w = 1.0 / np.clip(counts, 1, None)\n",
    "weights = class_w[np.array(targets)[tr_idx]]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(tr_idx), replacement=True)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "num_classes = len(full.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d70239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Number of classes: 30\n",
      "✓ Total samples: 15000\n",
      "✓ Train samples: 12000\n",
      "✓ Val samples: 3000\n",
      "✓ No overlap: True\n",
      "✓ Duplicate images in sample (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset is loaded correctly\n",
    "print(f\"✓ Number of classes: {num_classes}\")\n",
    "print(f\"✓ Total samples: {len(full.samples)}\")\n",
    "print(f\"✓ Train samples: {len(tr_idx)}\")\n",
    "print(f\"✓ Val samples: {len(va_idx)}\")\n",
    "print(f\"✓ No overlap: {set(tr_idx).isdisjoint(set(va_idx))}\")\n",
    "\n",
    "# Quick check for duplicates (sample first 100 files)\n",
    "from hashlib import md5\n",
    "def quick_hash_check(indices, n=100):\n",
    "    hashes = set()\n",
    "    for i in indices[:n]:\n",
    "        path = full.samples[i][0]\n",
    "        with open(path, 'rb') as f:\n",
    "            hashes.add(md5(f.read()).hexdigest())\n",
    "    return hashes\n",
    "\n",
    "train_hashes = quick_hash_check(tr_idx.tolist())\n",
    "val_hashes = quick_hash_check(va_idx.tolist())\n",
    "duplicates = train_hashes & val_hashes\n",
    "print(f\"✓ Duplicate images in sample (should be 0): {len(duplicates)}\")\n",
    "\n",
    "if num_classes == 1:\n",
    "    print(\"⚠️  WARNING: Only 1 class detected - check dataset path!\")\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"⚠️  WARNING: Found {len(duplicates)} duplicate images between train/val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363e75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm, torch\n",
    "from torch import nn\n",
    "\n",
    "def make_model(name, num_classes):\n",
    "    m = timm.create_model(name, pretrained=True, drop_rate=0.2, drop_path_rate=0.1, num_classes=num_classes)\n",
    "    return m\n",
    "\n",
    "m_small = make_model(\"mobilenetv3_small_100\", num_classes)\n",
    "m_large = make_model(\"mobilenetv3_large_100\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20805df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_dl, val_dl, epochs=15, lr=5e-4, wd=0.05, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=3)\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs-3)\n",
    "    sched = torch.optim.lr_scheduler.SequentialLR(opt, [warmup, cosine], milestones=[3])\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scaler = GradScaler('cuda', enabled=(device.startswith(\"cuda\")))\n",
    "    best = {\"f1\": -1, \"state\": None}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for x,y in train_dl:\n",
    "            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast('cuda', enabled=(device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                loss = crit(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        sched.step()\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_dl:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                logits = model(x)\n",
    "                preds.append(logits.argmax(1).cpu())\n",
    "                gts.append(y)\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import f1_score, accuracy_score\n",
    "        p = torch.cat(preds).numpy(); g = torch.cat(gts).numpy()\n",
    "        f1 = f1_score(g, p, average=\"macro\"); acc = accuracy_score(g, p)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"f1\": f1, \"state\": model.state_dict()}\n",
    "        print(f\"ep {ep+1}: acc {acc:.4f}  macroF1 {f1:.4f}\")\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d961af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1: acc 0.5730  macroF1 0.5655\n",
      "ep 2: acc 0.7160  macroF1 0.7138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg_pj\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.7180  macroF1 0.7179\n",
      "ep 4: acc 0.7127  macroF1 0.7080\n",
      "ep 5: acc 0.7433  macroF1 0.7450\n",
      "ep 6: acc 0.7690  macroF1 0.7690\n",
      "ep 7: acc 0.7093  macroF1 0.7050\n",
      "ep 8: acc 0.7857  macroF1 0.7859\n",
      "ep 9: acc 0.7993  macroF1 0.8001\n",
      "ep 10: acc 0.8080  macroF1 0.8107\n",
      "ep 11: acc 0.8357  macroF1 0.8357\n",
      "ep 12: acc 0.8457  macroF1 0.8439\n",
      "ep 13: acc 0.8400  macroF1 0.8398\n",
      "ep 14: acc 0.8587  macroF1 0.8582\n",
      "ep 15: acc 0.8580  macroF1 0.8580\n",
      "ep 16: acc 0.8607  macroF1 0.8599\n",
      "ep 17: acc 0.8673  macroF1 0.8667\n",
      "ep 18: acc 0.8627  macroF1 0.8631\n",
      "ep 19: acc 0.8600  macroF1 0.8602\n",
      "ep 20: acc 0.8623  macroF1 0.8625\n",
      "ep 1: acc 0.5967  macroF1 0.5952\n",
      "ep 2: acc 0.7850  macroF1 0.7834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads_from_edge\\engg_pj\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: acc 0.8363  macroF1 0.8367\n",
      "ep 4: acc 0.8373  macroF1 0.8369\n",
      "ep 5: acc 0.8597  macroF1 0.8586\n",
      "ep 6: acc 0.8637  macroF1 0.8633\n",
      "ep 7: acc 0.8597  macroF1 0.8592\n",
      "ep 8: acc 0.8667  macroF1 0.8672\n",
      "ep 9: acc 0.8707  macroF1 0.8694\n",
      "ep 10: acc 0.8733  macroF1 0.8726\n",
      "ep 11: acc 0.8843  macroF1 0.8835\n",
      "ep 12: acc 0.8793  macroF1 0.8788\n",
      "ep 13: acc 0.8783  macroF1 0.8778\n",
      "ep 14: acc 0.8803  macroF1 0.8802\n",
      "ep 15: acc 0.8820  macroF1 0.8812\n",
      "ep 16: acc 0.8793  macroF1 0.8793\n",
      "ep 17: acc 0.8840  macroF1 0.8835\n",
      "ep 18: acc 0.8823  macroF1 0.8818\n",
      "ep 19: acc 0.8810  macroF1 0.8804\n",
      "ep 20: acc 0.8840  macroF1 0.8840\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "small = train_model(m_small, train_dl, val_dl, epochs=20, device=device)\n",
    "large = train_model(m_large, train_dl, val_dl, epochs=20, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caeb5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small ms/img: 11.293988227844238\n",
      "Large ms/img: 12.904634475708008\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "def ms_per_image(model, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    x = torch.randn(1,3,224,224, device=device)\n",
    "    # warmup\n",
    "    for _ in range(5): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(50): model(x)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "    return 1000*(time.time()-t0)/50\n",
    "\n",
    "print(\"Small ms/img:\", ms_per_image(small, device))\n",
    "print(\"Large ms/img:\", ms_per_image(large, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c5a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\":\"mobilenetv3_small_100\",\"classes\":full.classes,\"state_dict\":small.state_dict()}, \"mobilenetv3_small.pt\")\n",
    "\n",
    "# ONNX for CPU apps\n",
    "dummy = torch.randn(1,3,224,224)\n",
    "small.eval().cpu()\n",
    "torch.onnx.export(small, dummy, \"mobilenetv3_small.onnx\", input_names=[\"input\"], output_names=[\"logits\"], opset_version=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ddbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
