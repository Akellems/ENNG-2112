{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RealWaste Dataset Training\n",
        "Training MobileNetV3-Small and ViT-Small models on the RealWaste dataset (9 classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Data Loading and Preprocessing\n",
        "from __future__ import annotations\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional, Dict\n",
        "import platform\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed: int = 56) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_num_workers(requested_workers: int = 2) -> int:\n",
        "    \"\"\"Windows-safe num_workers configuration\"\"\"\n",
        "    if platform.system() == 'Windows':\n",
        "        print(f\"[Platform] Windows detected: Using num_workers=0\")\n",
        "        return 0\n",
        "    return requested_workers\n",
        "\n",
        "set_seed(56)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load RealWaste Dataset\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "# Point to RealWaste dataset\n",
        "DATA_ROOT = Path(\"realwaste/realwaste-main/RealWaste\")\n",
        "\n",
        "if not DATA_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Dataset path does not exist: {DATA_ROOT}\")\n",
        "\n",
        "print(f\"Using dataset root: {DATA_ROOT}\")\n",
        "\n",
        "# Filter for JPG images (RealWaste uses .jpg)\n",
        "def is_jpg_image(p):\n",
        "    return str(p).lower().endswith(\".jpg\")\n",
        "\n",
        "# Load full dataset to get class info\n",
        "full = datasets.ImageFolder(root=str(DATA_ROOT), transform=None, is_valid_file=is_jpg_image)\n",
        "print(f\"Found {len(full.classes)} classes: {full.classes}\")\n",
        "print(f\"Total images: {len(full.samples)}\")\n",
        "\n",
        "# Print class distribution\n",
        "targets = [y for _, y in full.samples]\n",
        "print(\"\\nClass distribution:\")\n",
        "for i, class_name in enumerate(full.classes):\n",
        "    count = sum(1 for t in targets if t == i)\n",
        "    print(f\"  {class_name:25s}: {count:4d} images\")\n",
        "\n",
        "# Stratified split (70% train, 20% val, 10% test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: separate test set (10%)\n",
        "tr_val_idx, te_idx = train_test_split(\n",
        "    np.arange(len(targets)), test_size=0.1, random_state=56, stratify=targets\n",
        ")\n",
        "\n",
        "# Second split: train/val from remaining (70/20 of original = ~77.8/22.2 of remaining)\n",
        "tr_targets = [targets[i] for i in tr_val_idx]\n",
        "tr_idx, va_idx = train_test_split(\n",
        "    tr_val_idx, test_size=0.222, random_state=56, stratify=tr_targets  # 0.222 * 0.9 ≈ 0.2\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Train samples: {len(tr_idx)} ({len(tr_idx)/len(targets)*100:.1f}%)\")\n",
        "print(f\"✓ Val samples: {len(va_idx)} ({len(va_idx)/len(targets)*100:.1f}%)\")\n",
        "print(f\"✓ Test samples: {len(te_idx)} ({len(te_idx)/len(targets)*100:.1f}%)\")\n",
        "print(f\"✓ No overlap: {len(set(tr_idx) & set(va_idx) & set(te_idx)) == 0}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create transforms and datasets\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "])\n",
        "\n",
        "# Create datasets with appropriate transforms\n",
        "train_ds = Subset(\n",
        "    datasets.ImageFolder(str(DATA_ROOT), transform=train_tfms, is_valid_file=is_jpg_image),\n",
        "    tr_idx\n",
        ")\n",
        "val_ds = Subset(\n",
        "    datasets.ImageFolder(str(DATA_ROOT), transform=val_tfms, is_valid_file=is_jpg_image),\n",
        "    va_idx\n",
        ")\n",
        "test_ds = Subset(\n",
        "    datasets.ImageFolder(str(DATA_ROOT), transform=val_tfms, is_valid_file=is_jpg_image),\n",
        "    te_idx\n",
        ")\n",
        "\n",
        "num_classes = len(full.classes)\n",
        "print(f\"\\n✓ Number of classes: {num_classes}\")\n",
        "print(f\"✓ Classes: {full.classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create weighted sampler for class balancing\n",
        "train_targets = np.array([targets[i] for i in tr_idx])\n",
        "counts = np.bincount(train_targets)\n",
        "class_weights = 1.0 / np.clip(counts, 1, None)\n",
        "sample_weights = class_weights[train_targets]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(tr_idx), replacement=True)\n",
        "\n",
        "print(f\"✓ Class distribution (train) with weights:\")\n",
        "for i, cls in enumerate(full.classes):\n",
        "    print(f\"  {cls:25s}: {counts[i]:4d} samples (weight: {class_weights[i]:.3f})\")\n",
        "\n",
        "# Create DataLoaders\n",
        "num_workers = get_num_workers(4)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, sampler=sampler, \n",
        "                      num_workers=num_workers, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=128, shuffle=False, \n",
        "                    num_workers=num_workers, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=128, shuffle=False,\n",
        "                     num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(f\"\\n✓ DataLoaders created (num_workers={num_workers})\")\n",
        "print(f\"  Train batches: {len(train_dl)}\")\n",
        "print(f\"  Val batches: {len(val_dl)}\")\n",
        "print(f\"  Test batches: {len(test_dl)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Models\n",
        "import timm\n",
        "from torch import nn\n",
        "\n",
        "def make_model(name, num_classes):\n",
        "    \"\"\"Create a pretrained model with dropout regularization\"\"\"\n",
        "    m = timm.create_model(name, pretrained=True, drop_rate=0.2, drop_path_rate=0.1, num_classes=num_classes)\n",
        "    return m\n",
        "\n",
        "# Create MobileNetV3-Small and ViT-Small\n",
        "mobilenet_small = make_model(\"mobilenetv3_small_100\", num_classes)\n",
        "vit_small = make_model(\"vit_small_patch16_224\", num_classes)\n",
        "\n",
        "print(f\"✓ MobileNetV3-Small params: {sum(p.numel() for p in mobilenet_small.parameters())/1e6:.2f}M\")\n",
        "print(f\"✓ ViT-Small params: {sum(p.numel() for p in vit_small.parameters())/1e6:.2f}M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Function with Mixed Precision\n",
        "from torch.amp import autocast, GradScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def train_model(model, train_dl, val_dl, epochs=20, lr=5e-4, wd=0.05, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Train a model with:\n",
        "    - Mixed precision training\n",
        "    - AdamW optimizer with weight decay\n",
        "    - Linear warmup (3 epochs) + Cosine annealing\n",
        "    - Label smoothing\n",
        "    - Best model checkpointing based on macro F1\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    \n",
        "    # Learning rate schedule: warmup + cosine\n",
        "    warmup = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=3)\n",
        "    cosine = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs-3)\n",
        "    sched = torch.optim.lr_scheduler.SequentialLR(opt, [warmup, cosine], milestones=[3])\n",
        "    \n",
        "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    scaler = GradScaler('cuda', enabled=(device.startswith(\"cuda\")))\n",
        "    best = {\"f1\": -1, \"state\": None, \"epoch\": 0}\n",
        "    \n",
        "    for ep in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x, y in train_dl:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            \n",
        "            with autocast('cuda', enabled=(device.startswith(\"cuda\"))):\n",
        "                logits = model(x)\n",
        "                loss = crit(logits, y)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        sched.step()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        preds, gts = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_dl:\n",
        "                x = x.to(device, non_blocking=True)\n",
        "                logits = model(x)\n",
        "                preds.append(logits.argmax(1).cpu())\n",
        "                gts.append(y)\n",
        "        \n",
        "        p = torch.cat(preds).numpy()\n",
        "        g = torch.cat(gts).numpy()\n",
        "        f1 = f1_score(g, p, average=\"macro\")\n",
        "        acc = accuracy_score(g, p)\n",
        "        \n",
        "        # Save best model\n",
        "        if f1 > best[\"f1\"]:\n",
        "            best = {\"f1\": f1, \"state\": model.state_dict(), \"epoch\": ep+1}\n",
        "        \n",
        "        print(f\"ep {ep+1:2d}: acc {acc:.4f}  macroF1 {f1:.4f}  (best F1: {best['f1']:.4f} @ ep{best['epoch']})\")\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(best[\"state\"])\n",
        "    print(f\"\\n✓ Training complete. Best model from epoch {best['epoch']} (F1: {best['f1']:.4f})\")\n",
        "    return model\n",
        "\n",
        "print(\"✓ Training function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check CUDA availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"Device: CPU\")\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MobileNetV3-Small\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING MOBILENETV3-SMALL\")\n",
        "print(\"=\"*60)\n",
        "mobilenet_trained = train_model(mobilenet_small, train_dl, val_dl, epochs=20, lr=5e-4, wd=0.05, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train ViT-Small\n",
        "# ViT typically needs slightly lower learning rate\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING VIT-SMALL\")\n",
        "print(\"=\"*60)\n",
        "vit_trained = train_model(vit_small, train_dl, val_dl, epochs=20, lr=3e-4, wd=0.05, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Comparison and Benchmarking\n",
        "import time\n",
        "\n",
        "def ms_per_image(model, device=\"cuda\"):\n",
        "    \"\"\"Measure inference speed in milliseconds per image\"\"\"\n",
        "    model.eval().to(device)\n",
        "    x = torch.randn(1, 3, 224, 224, device=device)\n",
        "    \n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        model(x)\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    # Benchmark\n",
        "    t0 = time.time()\n",
        "    for _ in range(50):\n",
        "        model(x)\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    return 1000 * (time.time() - t0) / 50\n",
        "\n",
        "def get_final_metrics(model, val_dl, device):\n",
        "    \"\"\"Get final accuracy and F1 score\"\"\"\n",
        "    model.eval().to(device)\n",
        "    preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            preds.append(logits.argmax(1).cpu())\n",
        "            gts.append(y)\n",
        "    \n",
        "    p = torch.cat(preds).numpy()\n",
        "    g = torch.cat(gts).numpy()\n",
        "    return accuracy_score(g, p), f1_score(g, p, average=\"macro\")\n",
        "\n",
        "# Inference speed comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INFERENCE SPEED COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "mobilenet_speed = ms_per_image(mobilenet_trained, device)\n",
        "vit_speed = ms_per_image(vit_trained, device)\n",
        "print(f\"MobileNetV3-Small: {mobilenet_speed:.2f} ms/image\")\n",
        "print(f\"ViT-Small:         {vit_speed:.2f} ms/image\")\n",
        "print(f\"Speed ratio:       {vit_speed/mobilenet_speed:.2f}x (ViT/MobileNet)\")\n",
        "\n",
        "# Final validation performance\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL VALIDATION PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "models = [\n",
        "    (\"MobileNetV3-Small\", mobilenet_trained),\n",
        "    (\"ViT-Small\", vit_trained)\n",
        "]\n",
        "\n",
        "results = []\n",
        "for name, model in models:\n",
        "    acc, f1 = get_final_metrics(model, val_dl, device)\n",
        "    params = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "    speed = ms_per_image(model, device)\n",
        "    results.append((name, acc, f1, params, speed))\n",
        "    print(f\"{name:20s} | Acc: {acc:.4f} | F1: {f1:.4f} | Params: {params:.2f}M | Speed: {speed:.2f}ms\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Set Evaluation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for name, model in models:\n",
        "    acc, f1 = get_final_metrics(model, test_dl, device)\n",
        "    print(f\"{name:20s} | Test Acc: {acc:.4f} | Test F1: {f1:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Models\n",
        "# Save MobileNetV3-Small in PyTorch and ONNX formats\n",
        "torch.save({\n",
        "    \"model\": \"mobilenetv3_small_100\",\n",
        "    \"classes\": full.classes,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"state_dict\": mobilenet_trained.state_dict(),\n",
        "    \"dataset\": \"RealWaste\"\n",
        "}, \"mobilenetv3_small_realwaste.pt\")\n",
        "\n",
        "# Export to ONNX for deployment\n",
        "dummy = torch.randn(1, 3, 224, 224)\n",
        "mobilenet_trained.eval().cpu()\n",
        "torch.onnx.export(\n",
        "    mobilenet_trained, \n",
        "    dummy, \n",
        "    \"mobilenetv3_small_realwaste.onnx\",\n",
        "    input_names=[\"input\"], \n",
        "    output_names=[\"logits\"], \n",
        "    opset_version=17,\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}}\n",
        ")\n",
        "\n",
        "# Save ViT-Small\n",
        "torch.save({\n",
        "    \"model\": \"vit_small_patch16_224\",\n",
        "    \"classes\": full.classes,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"state_dict\": vit_trained.state_dict(),\n",
        "    \"dataset\": \"RealWaste\"\n",
        "}, \"vit_small_realwaste.pt\")\n",
        "\n",
        "print(\"✓ Saved MobileNetV3-Small:\")\n",
        "print(\"  - mobilenetv3_small_realwaste.pt (PyTorch)\")\n",
        "print(\"  - mobilenetv3_small_realwaste.onnx (ONNX)\")\n",
        "print(\"✓ Saved ViT-Small:\")\n",
        "print(\"  - vit_small_realwaste.pt (PyTorch)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Confusion Matrix Analysis\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(model, val_dl, classes, device, title=\"Confusion Matrix\"):\n",
        "    \"\"\"Plot confusion matrix for model predictions\"\"\"\n",
        "    model.eval().to(device)\n",
        "    preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            preds.append(logits.argmax(1).cpu())\n",
        "            gts.append(y)\n",
        "    \n",
        "    p = torch.cat(preds).numpy()\n",
        "    g = torch.cat(gts).numpy()\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(g, p)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\n{title} - Classification Report:\")\n",
        "    print(classification_report(g, p, target_names=classes, digits=4))\n",
        "\n",
        "# Plot for both models\n",
        "print(\"=\"*60)\n",
        "print(\"CONFUSION MATRIX ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "plot_confusion_matrix(mobilenet_trained, val_dl, full.classes, device, \n",
        "                     title=\"MobileNetV3-Small Confusion Matrix\")\n",
        "plot_confusion_matrix(vit_trained, val_dl, full.classes, device, \n",
        "                     title=\"ViT-Small Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Summary\n",
        "\n",
        "This notebook trains two models on the **RealWaste dataset** (9 classes of waste):\n",
        "- **Cardboard, Food Organics, Glass, Metal, Miscellaneous Trash, Paper, Plastic, Textile Trash, Vegetation**\n",
        "\n",
        "### Models Trained:\n",
        "1. **MobileNetV3-Small** (~1.5M params) - Efficient mobile-optimized CNN\n",
        "2. **ViT-Small** (~22M params) - Vision Transformer with attention mechanism\n",
        "\n",
        "### Training Configuration:\n",
        "- **Optimizer**: AdamW with weight decay (0.05)\n",
        "- **Learning Rate**: 5e-4 (MobileNet), 3e-4 (ViT)\n",
        "- **LR Schedule**: Linear warmup (3 epochs) + Cosine annealing\n",
        "- **Data Augmentation**: RandomResizedCrop, HorizontalFlip, ColorJitter\n",
        "- **Regularization**: Dropout (0.2), DropPath (0.1), Label smoothing (0.1)\n",
        "- **Training**: Mixed precision (FP16) with gradient scaling\n",
        "- **Epochs**: 20\n",
        "- **Batch Size**: 64 (train), 128 (val/test)\n",
        "- **Class Balancing**: Weighted random sampling\n",
        "- **Data Split**: 70% train, 20% val, 10% test (stratified)\n",
        "\n",
        "### Dataset Characteristics:\n",
        "- **Total Images**: ~4,752 JPG images\n",
        "- **Classes**: 9 waste categories\n",
        "- **Image Format**: RGB JPG files\n",
        "- **Source**: Local RealWaste dataset\n",
        "\n",
        "### Expected Performance:\n",
        "- **MobileNetV3-Small**: ~80-85% accuracy, fast inference (~5-7ms)\n",
        "- **ViT-Small**: ~85-90% accuracy, moderate inference (~5-6ms)\n",
        "\n",
        "### Output Files:\n",
        "- `mobilenetv3_small_realwaste.pt` - PyTorch checkpoint\n",
        "- `mobilenetv3_small_realwaste.onnx` - ONNX export for deployment\n",
        "- `vit_small_realwaste.pt` - PyTorch checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Test Loading Saved Models\n",
        "# This cell demonstrates how to load and use the saved models\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path, device=\"cuda\"):\n",
        "    \"\"\"Load a trained model from checkpoint\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    \n",
        "    # Recreate model\n",
        "    model = timm.create_model(\n",
        "        checkpoint[\"model\"], \n",
        "        pretrained=False,\n",
        "        num_classes=checkpoint[\"num_classes\"]\n",
        "    )\n",
        "    \n",
        "    # Load weights\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    return model, checkpoint[\"classes\"]\n",
        "\n",
        "# Example: Load MobileNetV3-Small\n",
        "print(\"Testing model loading...\")\n",
        "loaded_model, classes = load_model_from_checkpoint(\"mobilenetv3_small_realwaste.pt\", device)\n",
        "print(f\"✓ Loaded MobileNetV3-Small\")\n",
        "print(f\"  Classes: {classes}\")\n",
        "\n",
        "# Quick test inference\n",
        "test_input = torch.randn(1, 3, 224, 224, device=device)\n",
        "with torch.no_grad():\n",
        "    output = loaded_model(test_input)\n",
        "    pred_class = output.argmax(1).item()\n",
        "    print(f\"  Test inference: predicted class {pred_class} ({classes[pred_class]})\")\n",
        "\n",
        "print(\"\\n✓ Model loading and inference working correctly!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Per-Class Performance Analysis\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def analyze_per_class_performance(model, val_dl, classes, device):\n",
        "    \"\"\"Analyze per-class precision, recall, and F1-score\"\"\"\n",
        "    model.eval().to(device)\n",
        "    preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            preds.append(logits.argmax(1).cpu())\n",
        "            gts.append(y)\n",
        "    \n",
        "    p = torch.cat(preds).numpy()\n",
        "    g = torch.cat(gts).numpy()\n",
        "    \n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(g, p, average=None)\n",
        "    \n",
        "    print(f\"\\nPer-Class Performance:\")\n",
        "    print(f\"{'Class':<25s} | {'Precision':>9s} | {'Recall':>9s} | {'F1-Score':>9s} | {'Support':>7s}\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, cls in enumerate(classes):\n",
        "        print(f\"{cls:<25s} | {precision[i]:9.4f} | {recall[i]:9.4f} | {f1[i]:9.4f} | {support[i]:7d}\")\n",
        "    \n",
        "    # Overall metrics\n",
        "    macro_p, macro_r, macro_f1 = precision.mean(), recall.mean(), f1.mean()\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Macro Average':<25s} | {macro_p:9.4f} | {macro_r:9.4f} | {macro_f1:9.4f} | {support.sum():7d}\")\n",
        "\n",
        "# Analyze both models\n",
        "print(\"=\"*80)\n",
        "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MobileNetV3-Small\")\n",
        "print(\"=\"*80)\n",
        "analyze_per_class_performance(mobilenet_trained, val_dl, full.classes, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ViT-Small\")\n",
        "print(\"=\"*80)\n",
        "analyze_per_class_performance(vit_trained, val_dl, full.classes, device)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
