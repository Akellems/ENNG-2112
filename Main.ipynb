{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4743001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e3439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed4ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, io\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643cfc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32d43f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "Built with CUDA: 12.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)          # e.g., 2.x.x+cu124\n",
    "print(\"Built with CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef33f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility + device\n",
    "SEED = 56\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd17c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if a graphics card (GPU) is available for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb4b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded to: C:\\Users\\angus\\.cache\\kagglehub\\datasets\\alistairking\\recyclable-and-household-waste-classification\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download dataset (Kaggle)\n",
    "# Dataset: alistairking/recyclable-and-household-waste-classification\n",
    "print(\"Downloading dataset...\")\n",
    "ds_path = kagglehub.dataset_download(\n",
    "    \"alistairking/recyclable-and-household-waste-classification\"\n",
    ")\n",
    "print(\"Dataset downloaded to:\", ds_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f64c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image root folder found: C:\\Users\\angus\\.cache\\kagglehub\\datasets\\alistairking\\recyclable-and-household-waste-classification\\versions\\1\\images\\images\n"
     ]
    }
   ],
   "source": [
    "# Some Kaggle datasets have extra folders like \"images/images\"\n",
    "# This loop finds the real folder that contains all the image categories.\n",
    "candidates = [\n",
    "    Path(ds_path) / \"images\" / \"images\",\n",
    "    Path(ds_path) / \"images\",\n",
    "    Path(ds_path),\n",
    "]\n",
    "\n",
    "DATA_ROOT = None\n",
    "for c in candidates:\n",
    "    if c.exists() and any(d.is_dir() for d in c.iterdir()):\n",
    "        DATA_ROOT = c\n",
    "        break\n",
    "\n",
    "if DATA_ROOT is None:\n",
    "    raise FileNotFoundError(f\"Could not find image folder in: {path}\")\n",
    "\n",
    "print(\"Image root folder found:\", DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508c7db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (30): ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste'] ...\n",
      "Total PNG images: 15000\n"
     ]
    }
   ],
   "source": [
    "# Build full ImageFolder (PNG-only filter)\n",
    "png_only = lambda p: str(p).lower().endswith(\".png\")\n",
    "full_ds = datasets.ImageFolder(root=str(DATA_ROOT), transform=None, is_valid_file=png_only)\n",
    "class_names: List[str] = full_ds.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes ({num_classes}):\", class_names[:10], \"...\" if num_classes > 10 else \"\")\n",
    "print(\"Total PNG images:\", len(full_ds.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f77e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories found: 30\n",
      "Example categories: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste']\n"
     ]
    }
   ],
   "source": [
    "# --- Check what categories (folders) exist ---\n",
    "# Each folder inside the dataset represents one type of waste.\n",
    "class_dirs = sorted([d.name for d in DATA_ROOT.iterdir() if d.is_dir()])\n",
    "print(\"Number of categories found:\", len(class_dirs))\n",
    "print(\"Example categories:\", class_dirs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6bad3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define image transformations (resizing and data augmentation) ---\n",
    "# These changes help prepare the photos before training the model.\n",
    "\n",
    "IMG_SIZE = 224  # final image size (in pixels)\n",
    "\n",
    "# Training transformations (adds small random changes for variety)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),             # make all images similar size\n",
    "    transforms.RandomResizedCrop(IMG_SIZE,     # randomly crop and resize\n",
    "                                 scale=(0.8, 1.0),\n",
    "                                 ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),         # randomly flip images left-right\n",
    "    transforms.RandomRotation(10),             # rotate slightly (±10°)\n",
    "    transforms.ColorJitter(                    # small colour adjustments\n",
    "        brightness=0.10, contrast=0.10, saturation=0.10, hue=0.05),\n",
    "    transforms.ToTensor(),                     # turn image into numeric array\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # standard colour scaling\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation/testing transformations (simpler, no random changes)\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Filter function: only include .png or .PNG images\n",
    "png_only = lambda p: str(p).lower().endswith(\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc9e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the dataset ---\n",
    "# Organises images into labelled groups using their folder names.\n",
    "full_ds = datasets.ImageFolder(\n",
    "    root=str(DATA_ROOT),\n",
    "    transform=None,          # we'll apply transforms later\n",
    "    is_valid_file=png_only   # only include PNG files\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e732792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of categories: 30\n",
      "First few category names: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste']\n",
      "Total number of images: 15000\n"
     ]
    }
   ],
   "source": [
    "# Print a quick summary\n",
    "num_classes = len(full_ds.classes)\n",
    "print(f\"Total number of categories: {num_classes}\")\n",
    "print(\"First few category names:\", full_ds.classes[:10])\n",
    "print(\"Total number of images:\", len(full_ds.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7bc0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7077017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train: 4043, val: 505, test: 506\n",
      "Classes (6): ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Batch shapes -> images: (32, 3, 224, 224), labels: (32,)\n"
     ]
    }
   ],
   "source": [
    "# preprocess_trashnet.py\n",
    "\"\"\"\n",
    "Preprocessing & DataLoaders for Hugging Face dataset: garythung/trashnet\n",
    "\n",
    "Fixes for the errors you hit:\n",
    "- Handle HuggingFace `set_transform` batch semantics (the transform receives lists).\n",
    "- Force RGB before resizing to avoid mixed channel counts.\n",
    "- Custom collate_fn that tolerates single-item lists from `set_transform`.\n",
    "- No Dataset.with_format('torch'); we stack tensors ourselves.\n",
    "- CLI uses parse_known_args() so Jupyter's injected --f arg won't crash.\n",
    "\n",
    "Quick use (Notebook/Python):\n",
    "    from preprocess_trashnet import build_dataloaders\n",
    "    loaders, classes = build_dataloaders(batch_size=32, image_size=224, num_workers=0)\n",
    "    xb, yb = next(iter(loaders[\"train\"]))\n",
    "    print(xb.shape, yb.shape, classes)\n",
    "\n",
    "CLI:\n",
    "    python preprocess_trashnet.py --batch-size 64 --image-size 224 --num-workers 0\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Features\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    image_size: int = 224\n",
    "    val_pct: float = 0.10\n",
    "    test_pct: float = 0.10\n",
    "    seed: int = 42\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 0  # safest on Windows; bump if you want\n",
    "    pin_memory: bool = torch.cuda.is_available()\n",
    "    persistent_workers: bool = False  # will enable if num_workers > 0\n",
    "    augment: bool = True\n",
    "    export_dir: Optional[str] = None  # optional: export class-folder JPGs\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# ---------- Utils ----------\n",
    "\n",
    "def _set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def _pick_base_split(ds: DatasetDict) -> Dataset:\n",
    "    return ds[\"train\"] if \"train\" in ds else ds[next(iter(ds.keys()))]\n",
    "\n",
    "\n",
    "def _stratified_splits(base: Dataset, val_pct: float, test_pct: float, seed: int) -> DatasetDict:\n",
    "    assert 0 < val_pct < 1 and 0 < test_pct < 1 and val_pct + test_pct < 1, \"Invalid val/test percentages.\"\n",
    "    holdout_pct = val_pct + test_pct\n",
    "    tmp = base.train_test_split(test_size=holdout_pct, stratify_by_column=\"label\", seed=seed)\n",
    "    train, holdout = tmp[\"train\"], tmp[\"test\"]\n",
    "    test_frac_of_holdout = test_pct / (val_pct + test_pct)\n",
    "    hold = holdout.train_test_split(test_size=test_frac_of_holdout, stratify_by_column=\"label\", seed=seed)\n",
    "    return DatasetDict(train=train, val=hold[\"train\"], test=hold[\"test\"])\n",
    "\n",
    "\n",
    "def _maybe_make_splits(ds: DatasetDict, cfg: Config) -> DatasetDict:\n",
    "    keys = set(ds.keys())\n",
    "    if {\"train\", \"validation\", \"test\"}.issubset(keys):\n",
    "        return DatasetDict(train=ds[\"train\"], val=ds[\"validation\"], test=ds[\"test\"])\n",
    "    if {\"train\", \"val\", \"test\"}.issubset(keys):\n",
    "        return DatasetDict(train=ds[\"train\"], val=ds[\"val\"], test=ds[\"test\"])\n",
    "    return _stratified_splits(_pick_base_split(ds), cfg.val_pct, cfg.test_pct, cfg.seed)\n",
    "\n",
    "\n",
    "def _build_transforms(cfg: Config):\n",
    "    # Force RGB first (some images may be grayscale)\n",
    "    common = [\n",
    "        T.Lambda(lambda im: im.convert(\"RGB\")),  # <- important\n",
    "        T.Resize((cfg.image_size, cfg.image_size), interpolation=InterpolationMode.BILINEAR),\n",
    "    ]\n",
    "    train_list = common + (\n",
    "        [\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomApply([T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)], p=0.5),\n",
    "            T.RandomAffine(degrees=12, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "        ] if cfg.augment else []\n",
    "    ) + [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]\n",
    "    eval_list = common + [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]\n",
    "    return T.Compose(train_list), T.Compose(eval_list)\n",
    "\n",
    "\n",
    "def _set_transform(ds: Dataset, tfm):\n",
    "    \"\"\"\n",
    "    HuggingFace `set_transform` applies on *batches*: the dict values are lists.\n",
    "    We must map the torchvision transform over each image in the list.\n",
    "    \"\"\"\n",
    "    def _apply(batch):\n",
    "        imgs = batch[\"image\"]\n",
    "        if not isinstance(imgs, list):  # be robust if it's a single example\n",
    "            imgs = [imgs]\n",
    "        batch[\"image\"] = [tfm(img) for img in imgs]\n",
    "        return batch\n",
    "    ds.set_transform(_apply)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _class_names_from_features(feats: Features) -> List[str]:\n",
    "    if \"label\" in feats and hasattr(feats[\"label\"], \"names\") and feats[\"label\"].names:\n",
    "        return list(feats[\"label\"].names)\n",
    "    return []\n",
    "\n",
    "\n",
    "def _export_split(split: Dataset, split_name: str, class_names: List[str], out_dir: str, image_size: int):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    export_tfm = T.Compose([\n",
    "        T.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "        T.Resize((image_size, image_size), interpolation=InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "    for idx in tqdm(range(len(split)), desc=f\"Exporting {split_name}\"):\n",
    "        ex = split[idx]\n",
    "        img: Image.Image = ex[\"image\"]\n",
    "        label = int(ex[\"label\"])\n",
    "        cls = class_names[label] if class_names else str(label)\n",
    "        cls_dir = os.path.join(out_dir, split_name, cls)\n",
    "        os.makedirs(cls_dir, exist_ok=True)\n",
    "        export_tfm(img).save(os.path.join(cls_dir, f\"{idx:06d}.jpg\"), format=\"JPEG\", quality=92, optimize=True)\n",
    "\n",
    "\n",
    "def _collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Each item `b` may come with b[\"image\"] as a Tensor OR a single-item list [Tensor]\n",
    "    depending on HF internals. Be tolerant.\n",
    "    \"\"\"\n",
    "    imgs, labels = [], []\n",
    "    for b in batch:\n",
    "        img = b[\"image\"]\n",
    "        if isinstance(img, list):\n",
    "            img = img[0]\n",
    "        lbl = b[\"label\"]\n",
    "        if isinstance(lbl, list):\n",
    "            lbl = lbl[0]\n",
    "        imgs.append(img)\n",
    "        labels.append(int(lbl))\n",
    "    return torch.stack(imgs, dim=0), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ---------- Public API ----------\n",
    "\n",
    "def build_dataloaders(\n",
    "    batch_size: int = 32,\n",
    "    image_size: int = 224,\n",
    "    val_pct: float = 0.10,\n",
    "    test_pct: float = 0.10,\n",
    "    seed: int = 42,\n",
    "    num_workers: Optional[int] = None,\n",
    "    augment: bool = True,\n",
    "    export_dir: Optional[str] = None,\n",
    ") -> Tuple[Dict[str, DataLoader], List[str]]:\n",
    "    cfg = Config(\n",
    "        image_size=image_size,\n",
    "        val_pct=val_pct,\n",
    "        test_pct=test_pct,\n",
    "        seed=seed,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=(num_workers if num_workers is not None else 0),\n",
    "        augment=augment,\n",
    "        export_dir=export_dir,\n",
    "    )\n",
    "    cfg.persistent_workers = bool(cfg.num_workers and cfg.num_workers > 0)\n",
    "\n",
    "    _set_seed(cfg.seed)\n",
    "\n",
    "    # 1) Load dataset\n",
    "    ds = load_dataset(\"garythung/trashnet\")\n",
    "\n",
    "    # 2) Standardize splits\n",
    "    splits = _maybe_make_splits(ds, cfg)\n",
    "\n",
    "    # 3) Class names\n",
    "    feats = _pick_base_split(ds).features\n",
    "    class_names = _class_names_from_features(feats)\n",
    "\n",
    "    # 4) Transforms\n",
    "    train_tfm, eval_tfm = _build_transforms(cfg)\n",
    "\n",
    "    # 5) Attach transforms (batch-aware)\n",
    "    splits[\"train\"] = _set_transform(splits[\"train\"], train_tfm)\n",
    "    splits[\"val\"]   = _set_transform(splits[\"val\"],   eval_tfm)\n",
    "    splits[\"test\"]  = _set_transform(splits[\"test\"],  eval_tfm)\n",
    "\n",
    "    # 6) Optional export (use a fresh view without transforms)\n",
    "    if cfg.export_dir:\n",
    "        raw = _maybe_make_splits(ds, cfg)\n",
    "        for part in [\"train\", \"val\", \"test\"]:\n",
    "            _export_split(raw[part], part, class_names, cfg.export_dir, cfg.image_size)\n",
    "\n",
    "    # 7) DataLoaders\n",
    "    loaders: Dict[str, DataLoader] = {\n",
    "        \"train\": DataLoader(\n",
    "            splits[\"train\"],\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=cfg.pin_memory,\n",
    "            persistent_workers=cfg.persistent_workers,\n",
    "            collate_fn=_collate_batch,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            splits[\"val\"],\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=cfg.pin_memory,\n",
    "            persistent_workers=cfg.persistent_workers,\n",
    "            collate_fn=_collate_batch,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            splits[\"test\"],\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=cfg.pin_memory,\n",
    "            persistent_workers=cfg.persistent_workers,\n",
    "            collate_fn=_collate_batch,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return loaders, class_names\n",
    "\n",
    "\n",
    "# ---------- CLI ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocess garythung/trashnet and build loaders / optional export.\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--image-size\", type=int, default=224)\n",
    "    parser.add_argument(\"--val-pct\", type=float, default=0.10)\n",
    "    parser.add_argument(\"--test-pct\", type=float, default=0.10)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=0)\n",
    "    parser.add_argument(\"--no-augment\", action=\"store_true\", help=\"Disable training-time data augmentation.\")\n",
    "    parser.add_argument(\"--export-dir\", type=str, default=None, help=\"If set, saves resized JPGs to this folder.\")\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "\n",
    "    loaders, class_names = build_dataloaders(\n",
    "        batch_size=args.batch_size,\n",
    "        image_size=args.image_size,\n",
    "        val_pct=args.val_pct,\n",
    "        test_pct=args.test_pct,\n",
    "        seed=args.seed,\n",
    "        num_workers=args.num_workers,\n",
    "        augment=not args.no_augment,\n",
    "        export_dir=args.export_dir,\n",
    "    )\n",
    "\n",
    "    n_train = len(loaders[\"train\"].dataset)\n",
    "    n_val   = len(loaders[\"val\"].dataset)\n",
    "    n_test  = len(loaders[\"test\"].dataset)\n",
    "    print(f\"Splits -> train: {n_train}, val: {n_val}, test: {n_test}\")\n",
    "    if class_names:\n",
    "        print(f\"Classes ({len(class_names)}): {class_names}\")\n",
    "\n",
    "    xb, yb = next(iter(loaders[\"train\"]))\n",
    "    print(f\"Batch shapes -> images: {tuple(xb.shape)}, labels: {tuple(yb.shape)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
